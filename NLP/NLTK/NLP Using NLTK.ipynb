{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce03172d",
   "metadata": {},
   "source": [
    "### Dive into NLP with NLTK: A Beginner's Guide.\n",
    "</h>\n",
    "\n",
    "* What is NLP?\n",
    "\n",
    "NLP, or Natural Language Processing, is a field of computer science concerned with giving computers the ability to understand and work with human language. It combines techniques from linguistics and computer science to enable machines to process and analyze spoken or written text.\n",
    "\n",
    "The goal of NLP is to bridge the gap between human and machine communication. NLP allows computers to:\n",
    "1. Understand the meaning of text: Analyze the sentiment, intent, and context of language.\n",
    "2. Manipulate language: Tasks like generating text, translating languages, and summarizing information.\n",
    "\n",
    "This makes NLP crucial for applications like chatbots, machine translation, and sentiment analysis.\n",
    "\n",
    "* Why NLTK ?\n",
    "\n",
    "When it comes to Natural Language Processing (NLP) with Python, NLTK (Natural Language Toolkit) stands out as a popular choice for several reasons:\n",
    "\n",
    "1. Ease of Use: NLTK offers a user-friendly interface, making it accessible for beginners and experienced programmers alike. You don't need to be a linguistics expert to perform basic NLP tasks with NLTK.\n",
    "\n",
    "2. Rich Set of Tools: NLTK boasts a comprehensive toolkit for various NLP tasks. It provides functionalities for:\n",
    "\n",
    "3. Text processing: Break down sentences into words (tokenization), remove common words (stopwords), and convert words to their base form (stemming/lemmatization).\n",
    "4. Text analysis: Identify parts of speech (POS tagging) and grammatical structure (parsing).\n",
    "5. Semantic exploration: Utilize resources like WordNet to understand word relationships.\n",
    "6. Building applications: NLTK integrates with machine learning libraries, allowing you to build NLP applications like sentiment analysis or chatbots.\n",
    "6. Open Source and Community Driven:  Being free and open-source, NLTK benefits from a large and active developer community. This translates to extensive documentation, tutorials, and readily available pre-built datasets (corpora) for you to experiment with.\n",
    "\n",
    "NLTK's user-friendliness and rich set of tools make it a valuable asset for anyone venturing into the world of NLP with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2d8c8f",
   "metadata": {},
   "source": [
    "#### 2. Get Ready for NLP with Python!\n",
    "\n",
    "This guide will set up your environment for exploring the world of Natural Language Processing (NLP) with Python.\n",
    "\n",
    "1. Python Power:\n",
    "\n",
    "If you don't have Python installed yet, head over to the official download page: Python Download: https://www.python.org/downloads/. Download and install the latest version that suits your operating system.\n",
    "\n",
    "2. NLTK: Your NLP Toolkit:\n",
    "\n",
    "Once you have Python ready, let's install NLTK, a powerful library for NLP tasks. NLTK stands for Natural Language Toolkit.  Think of it as a toolbox filled with handy functions for working with human language.\n",
    "\n",
    "To install NLTK, open your terminal or command prompt and type:\n",
    "\n",
    "pip install nltk\n",
    "\n",
    "This command uses pip, Python's built-in package manager, to download and install NLTK for you. Now you're all set to dive into the exciting world of NLP with Python and NLTK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "383d42be-08a1-4f22-b0ef-aba464b0ce89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stemming\n",
      "  Downloading stemming-1.0.1.zip (13 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: stemming\n",
      "  Building wheel for stemming (pyproject.toml): started\n",
      "  Building wheel for stemming (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for stemming: filename=stemming-1.0.1-py3-none-any.whl size=11146 sha256=01cade9e412018ac58d780a5c607d559c61f291f875d96b6ec91e9ea511e71b3\n",
      "  Stored in directory: c:\\users\\firehouse technology\\appdata\\local\\pip\\cache\\wheels\\20\\d4\\73\\028ca44cd75949ad81250dd3ecea7e4c61b97672587b65ef35\n",
      "Successfully built stemming\n",
      "Installing collected packages: stemming\n",
      "Successfully installed stemming-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install stemming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ea55c7",
   "metadata": {},
   "source": [
    "### 3. Working with Text: Preprocessing Powerhouse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2345ff79",
   "metadata": {},
   "source": [
    "\n",
    "Now that you're equipped with Python and NLTK, let's explore some essential techniques for preparing text data for NLP tasks. These techniques are often referred to as text preprocessing.\n",
    "\n",
    "1. Tokenization: Cracking the Text Code\n",
    "\n",
    "Imagine a sentence as a code you need to decipher. Tokenization is the first step, where we break down this code into smaller, more manageable units. In NLP, these units are typically words or sentences.\n",
    "\n",
    "Here's a Python code snippet using NLTK to tokenize a sentence into words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "517ac900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Firehouse\n",
      "[nltk_data]     Technology\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Firehouse\n",
      "[nltk_data]     Technology\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Firehouse\n",
      "[nltk_data]     Technology\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download necessary resources from NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bc274e",
   "metadata": {},
   "source": [
    "In Natural Language Toolkit (NLTK), punkt is a tokenizer for English, while stopwords is a list of common English words that do not contain important meaning and are often removed in text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ed0cc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'is', 'fun', 'and', 'rewarding', '!']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample sentence\n",
    "text = \"Natural Language Processing is fun and rewarding!\"\n",
    "\n",
    "# Tokenization (splitting into words)\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8a125f",
   "metadata": {},
   "source": [
    "This code first imports the nltk library. Then, it defines a sample sentence and uses nltk.word_tokenize(text) to split the sentence into a list of words, stored in the tokens variable. Running this code will print the tokenized words, giving you a basic understanding of how the text is divided"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2f0ce6",
   "metadata": {},
   "source": [
    "2. Stopword Removal: Focusing on the Meaningful\n",
    "\n",
    "Not all words in a sentence carry the same weight. Some words, like \"the\", \"a\", \"an\", and \"is\", are common across many sentences and hold little meaning on their own. These are called stopwords.\n",
    "\n",
    "Removing stopwords helps focus on the words that truly contribute to the meaning of the text. Here's how to remove stopwords from our tokenized list using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6575cd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'fun', 'rewarding', '!']\n"
     ]
    }
   ],
   "source": [
    "# Stopwords list (pre-loaded in NLTK)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Remove stopwords from tokens\n",
    "filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed69a820",
   "metadata": {},
   "source": [
    "This code defines a list of stopwords using nltk.corpus.stopwords.words('english'). Then, it iterates through the tokens list and keeps only the words that are not present in the stopwords list. The resulting filtered_tokens list will contain the more meaningful words from the original sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de87b02",
   "metadata": {},
   "source": [
    "3. Stemming and Lemmatization: Unveiling the Root\n",
    "\n",
    "Words often have different variations based on tense or context. For example, \"running\", \"runs\", and \"ran\" all represent the same core concept of \"to run\".\n",
    "\n",
    "Stemming and lemmatization are techniques that reduce words to their base or root form. Stemming is a simpler approach that chops off suffixes without considering grammatical rules. Lemmatization takes a more linguistic approach, ensuring the resulting word is an actual word in the language.\n",
    "\n",
    "While both techniques have their advantages, lemmatization is generally preferred for better accuracy.  Here's a quick example (note that NLTK doesn't have built-in stemming functionality, but libraries like stemming can be used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ea221b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natur', 'Languag', 'Process', 'is', 'fun', 'and', 'reward', '!']\n"
     ]
    }
   ],
   "source": [
    "from stemming.porter2 import stem\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"Natural Language Processing is fun and rewarding!\"\n",
    "\n",
    "# Tokenize the sentence (assuming 'word_tokenize' is defined somewhere)\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Apply stemming to the tokens\n",
    "stemmed_tokens = [stem(token) for token in tokens]\n",
    "\n",
    "print(stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcae5323",
   "metadata": {},
   "source": [
    "This code imports the PorterStemmer class from the stemming library. It creates a stemmer object and uses a list comprehension to apply stemming to each token in the filtered_tokens list. The resulting stemmed_tokens list will contain the stemmed versions of the words.\n",
    "\n",
    "Lemmatization can be achieved using libraries like nltk.stem.WordNetLemmatizer. The approach is similar to stemming, but it considers the context to ensure the resulting word is a valid word.\n",
    "\n",
    "By applying these preprocessing techniques, you can transform raw text data into a cleaner and more focused format, making it easier for NLP models to understand and analyze the true meaning behind the words.\n",
    "\n",
    "All togeter you can reffer following example as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98fed251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: This is a sample text for NLP exploration. Let's remove stop words and make it lowercase.\n",
      "Tokens: ['This', 'is', 'a', 'sample', 'text', 'for', 'NLP', 'exploration', '.', 'Let', \"'s\", 'remove', 'stop', 'words', 'and', 'make', 'it', 'lowercase', '.']\n",
      "Filtered tokens (without stop words): ['This', 'sample', 'text', 'NLP', 'exploration', '.', 'Let', \"'s\", 'remove', 'stop', 'words', 'make', 'lowercase', '.']\n",
      "Lowercased tokens: ['this', 'sample', 'text', 'nlp', 'exploration', '.', 'let', \"'s\", 'remove', 'stop', 'words', 'make', 'lowercase', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Sample text\n",
    "text = \"This is a sample text for NLP exploration. Let's remove stop words and make it lowercase.\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Stop word removal (using English stop words)\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# Lowercasing\n",
    "lowercase_tokens = [token.lower() for token in filtered_tokens]\n",
    "\n",
    "# Print the results\n",
    "print(\"Original text:\", text)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Filtered tokens (without stop words):\", filtered_tokens)\n",
    "print(\"Lowercased tokens:\", lowercase_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e792b4",
   "metadata": {},
   "source": [
    "4. Parts-of-Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e88840",
   "metadata": {},
   "source": [
    "We've explored some fundamental text preprocessing techniques. Now, let's delve deeper into some powerful NLP tasks that extract valuable insights from text data.\n",
    "\n",
    "1. Part-of-Speech (POS) Tagging: Grammar Unveiled\n",
    "\n",
    "Imagine a sentence as a team, where each word plays a specific role. Part-of-Speech (POS) tagging helps us identify these roles. It assigns grammatical labels (tags) to each word, such as noun, verb, adjective, adverb, etc.\n",
    "\n",
    "By understanding the grammatical structure of a sentence, NLP models can gain a deeper comprehension of the relationships between words and the overall meaning of the text. Here's an example (code not shown, as POS tagging utilizes pre-trained models in NLTK):\n",
    "\n",
    "Consider the sentence: \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "POS tagging might assign tags like \"DT\" (determiner) to \"The\", \"JJ\" (adjective) to \"quick\", and \"NN\" (noun) to \"fox\". This grammatical breakdown allows NLP models to understand the sentence structure and relationships between words, like \"fox\" (subject) \"jumps\" (verb) and \"dog\" (object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57fb8fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK is a leading platform for building Python programs to work with human language data.', 'It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.']\n"
     ]
    }
   ],
   "source": [
    "# understanding sentence_tokenizer \n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\"\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fda526",
   "metadata": {},
   "source": [
    "this  tokenize the given text into a list of sentences. Each sentence will be a separate string element in the list. This is useful for tasks like sentence-level analysis or for breaking down a large body of text into smaller units for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ababd533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We've explored some fundamental text preprocessing techniques.\n",
      "[('We', 'PRP'), (\"'ve\", 'VBP'), ('explored', 'VBN'), ('some', 'DT'), ('fundamental', 'JJ'), ('text', 'NN'), ('preprocessing', 'VBG'), ('techniques', 'NNS'), ('.', '.')]\n",
      "\n",
      "Now, let's delve deeper into some powerful NLP tasks that extract valuable insights from text data.\n",
      "[('Now', 'RB'), (',', ','), ('let', 'VB'), (\"'s\", 'POS'), ('delve', 'VB'), ('deeper', 'NN'), ('into', 'IN'), ('some', 'DT'), ('powerful', 'JJ'), ('NLP', 'NNP'), ('tasks', 'NNS'), ('that', 'WDT'), ('extract', 'VBP'), ('valuable', 'JJ'), ('insights', 'NNS'), ('from', 'IN'), ('text', 'NN'), ('data', 'NNS'), ('.', '.')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"We've explored some fundamental text preprocessing techniques. \n",
    "Now, let's delve deeper into some powerful NLP tasks that extract valuable insights from text data.\"\"\"\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Loop through each sentence and perform POS tagging\n",
    "for sentence in sentences:\n",
    "  # Tokenize the sentence into words\n",
    "  \n",
    "  tokens = nltk.word_tokenize(sentence)\n",
    "  \n",
    "  # Perform POS tagging on the tokens\n",
    "  tags = nltk.pos_tag(tokens)\n",
    "  \n",
    "  # Print the sentence with corresponding POS tags\n",
    "  print(sentence)\n",
    "  print(tags)\n",
    "  print(\"\")  # Add an empty line for better readability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c84e9a",
   "metadata": {},
   "source": [
    "This code first imports necessary libraries: nltk for core NLP functionalities and sent_tokenize from nltk for splitting text into sentences.\n",
    "\n",
    "Then, it defines the sample text. The code uses sent_tokenize to split the text into individual sentences stored in the sentences list.\n",
    "\n",
    "It iterates through each sentence in the list. Inside the loop:\n",
    "\n",
    "Words are extracted from the sentence using nltk.word_tokenize.\n",
    "The nltk.pos_tag function is used to perform POS tagging on the tokens, resulting in a list of tuples where each tuple contains a word and its corresponding POS tag.\n",
    "Finally, both the original sentence and the list of POS tags are printed for each sentence. An empty line is added for better readability.\n",
    "Running this code will output the sample text along with the POS tags for each word, demonstrating how NLTK can be used for basic POS tagging tasks.\n",
    "\n",
    "Note: This example showcases POS tagging on pre-defined text. For real-world applications, you'll likely need to handle user input or read text from a file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b26f4a",
   "metadata": {},
   "source": [
    "##### Other NLTK capabilities (some of)\n",
    "\n",
    "2. Named Entity Recognition (NER): Spotting the Important Names\n",
    "\n",
    "Our world is full of named entities – people, organizations, locations, dates, monetary values, etc. Named Entity Recognition (NER) is the task of identifying and classifying these entities within text data.\n",
    "\n",
    "NER is crucial for various applications, such as information extraction, question answering, and content analysis. Here's an example (code not shown, as NER utilizes pre-trained models in NLTK):\n",
    "\n",
    "In the sentence: \"Barack Obama, the former president of the United States, visited India in 2023.\"\n",
    "\n",
    "NER might identify \"Barack Obama\" as a Person, \"United States\" as a Location, and \"2023\" as a Date. This extracted information can be used for various purposes, like building knowledge graphs or tracking mentions of specific entities in large datasets.\n",
    "\n",
    "3. Sentiment Analysis: Feeling the Text's Mood\n",
    "\n",
    "Have you ever read an online review and instantly grasped the writer's feeling about the product? Sentiment analysis is an NLP technique that aims to do just that – automatically determine the sentiment expressed in a piece of text, whether positive, negative, or neutral.\n",
    "\n",
    "Sentiment analysis is widely used for understanding customer feedback, monitoring social media brand mentions, and gauging public opinion on various topics. Here's an example (code not shown, as sentiment analysis often uses machine learning models):\n",
    "\n",
    "Consider the tweets:\n",
    "\n",
    "\"This movie was fantastic! A must-watch!\" (Positive sentiment)\n",
    "\"Absolutely disappointed with the service. Never again!\" (Negative sentiment)\n",
    "Sentiment analysis can classify these tweets based on the sentiment expressed, allowing businesses to gauge customer satisfaction or identify areas for improvement.\n",
    "\n",
    "By mastering these advanced NLP tasks, you can unlock a deeper understanding of the hidden meaning within text data. This empowers you to build intelligent applications that can interact with human language in increasingly sophisticated ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3add03b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04266687",
   "metadata": {},
   "source": [
    "5. Basic NLP Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6c8f6b",
   "metadata": {},
   "source": [
    "1. Word Frequency Analysis: Counting How Often Words Appear\n",
    "\n",
    "Imagine a bustling city street; some words, like \"the\" and \"a\", are like common pedestrians, appearing frequently. Others, like specific names or technical terms, are like rarer landmarks. Word frequency analysis counts the occurrences of each word in a text, revealing these usage patterns.\n",
    "\n",
    "Here's a Python code example using NLTK\n",
    "\n",
    "\n",
    "bellow code first imports nltk and stopwords from nltk.corpus. It defines the sample text. Then, it tokenizes the text, converting words to lowercase and removing stopwords using a list comprehension. Finally, it creates a frequency distribution using nltk.FreqDist(tokens). This fdist object keeps track of how many times each word appears in the text.\n",
    "\n",
    "The code then uses fdist.most_common(10) to retrieve the 10 most frequent words and their counts, providing insights into the most commonly used words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "423e13a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Most Frequent Words:\n",
      "[('language', 2), ('.', 2), ('natural', 1), ('processing', 1), ('rapidly', 1), ('growing', 1), ('field', 1), ('exciting', 1), ('applications', 1), ('it', 1)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Sample text\n",
    "text = \"Natural Language Processing is a rapidly growing field with exciting applications. It allows computers to understand and manipulate human language.\"\n",
    "\n",
    "# Tokenize the text (excluding stopwords)\n",
    "tokens = [word.lower() for word in nltk.word_tokenize(text) if word not in stopwords.words('english')]\n",
    "\n",
    "# Create a frequency distribution\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "\n",
    "# Print the 10 most frequent words\n",
    "print(\"10 Most Frequent Words:\")\n",
    "print(fdist.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67dcbb5",
   "metadata": {},
   "source": [
    "2. Concordance: Seeing Words in Context\n",
    "\n",
    "Frequency analysis tells us how often words appear, but concordance goes a step further. It shows us the contexts in which a specific word appears, providing a glimpse into its meaning and usage.\n",
    "\n",
    "Here's an example (code not shown, as concordance can be achieved using NLTK libraries):\n",
    "\n",
    "Imagine searching for the word \"applications\" in our sample text. A concordance might display snippets like:\n",
    "\n",
    "\"... exciting applications. It allows ...\"\n",
    "\"... understand and manipulate human language. It has various applications ...\"\n",
    "This context helps us understand how \"applications\" relates to other words like \"exciting\" and \"language,\" providing a deeper understanding of its usage within the text.\n",
    "\n",
    "3. Collocation: Unveiling Word Partnerships\n",
    "\n",
    "Words often travel in packs. Collocation analysis identifies words that frequently appear together, revealing potential semantic relationships.\n",
    "\n",
    "Here's a Python code example using NLTK (note: this is a simplified example, more advanced techniques exist for collocation analysis)\n",
    "\n",
    "\n",
    "This code builds upon the previous example's tokenization. It utilizes nltk.bigrams(tokens) to generate a list of word pairs (bigrams) from the tokens. Then, a frequency distribution is created for these bigrams using nltk.FreqDist(bigrams). Finally, it displays the 5 most frequent bigrams, revealing potential word partnerships within the text.\n",
    "\n",
    "By analyzing word frequency, concordance, and collocation, you can gain valuable insights into the vocabulary usage and thematic structure of your text data. This empowers you to build applications that can better understand the nuances of human language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "445601ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Most Frequent Bigrams:\n",
      "[(('natural', 'language'), 1), (('language', 'processing'), 1), (('processing', 'rapidly'), 1), (('rapidly', 'growing'), 1), (('growing', 'field'), 1)]\n"
     ]
    }
   ],
   "source": [
    "# Bigram extraction (considering word pairs)\n",
    "bigrams = list(nltk.bigrams(tokens))\n",
    "\n",
    "# Count occurrences of bigrams\n",
    "fdist_bigrams = nltk.FreqDist(bigrams)\n",
    "\n",
    "# Print the 5 most frequent bigrams\n",
    "print(\"5 Most Frequent Bigrams:\")\n",
    "print(fdist_bigrams.most_common(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a1588c",
   "metadata": {},
   "source": [
    "### Unveiling the Power of Text: A Journey with NLP and NLTK\n",
    "We've embarked on a thrilling exploration of Natural Language Processing (NLP) and its potential. We saw how NLTK, a powerful Python library, serves as a versatile toolkit for unlocking the meaning hidden within text data.\n",
    "\n",
    "1. ##### Recap: Our NLP Toolbox\n",
    "\n",
    "Our journey began with equipping ourselves for the task. We learned how to set up Python and NLTK, preparing the foundation for our NLP endeavors.\n",
    "\n",
    "Next, we delved into text preprocessing techniques – the groundwork for any NLP task. We explored tokenization, breaking down text into manageable units like words or sentences. We tackled stopword removal, focusing on the words that truly matter. Finally, we peeked into stemming and lemmatization, techniques for reducing words to their base forms.\n",
    "\n",
    "With a clean and prepped text, we ventured into advanced NLP tasks. We discovered Part-of-Speech (POS) tagging, which helps us understand the grammatical roles of words within a sentence. We explored Named Entity Recognition (NER), a technique for identifying and classifying important entities like names, locations, and dates. Finally, we unraveled the world of sentiment analysis, where we learned to determine the emotional tone expressed in a piece of text.\n",
    "\n",
    "2. ##### Unveiling More: The NLP Frontier Awaits\n",
    "\n",
    "This is just the beginning of your NLP adventure! NLTK offers a treasure trove of functionalities waiting to be explored. Here are some exciting areas to delve into next:\n",
    "\n",
    "* Stemming and Lemmatization: Dive deeper into these techniques for word normalization, ensuring a more consistent representation of words in your text data.\n",
    "\n",
    "* Sentiment Analysis: Refine your ability to gauge the sentiment expressed in text. Explore advanced libraries and techniques to analyze reviews, social media posts, or customer feedback.\n",
    "* Text Classification: Learn how to automatically categorize text documents into predefined classes, such as spam detection or topic classification. \n",
    "\n",
    "The NLTK documentation (https://www.nltk.org/book/) and numerous online tutorials offer a wealth of resources to guide you on your NLP journey. So, keep exploring, experiment with different techniques, and unlock the power of human language with NLP and NLTK!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
